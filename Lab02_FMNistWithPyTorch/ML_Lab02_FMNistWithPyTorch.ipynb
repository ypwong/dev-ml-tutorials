{"cells":[{"cell_type":"markdown","source":["# Machine Learning\n","## Lab02 - Let's PyTorch, Light the Flame of Learning Machine Learning\n","\n","Nevermind if you do not understand YET!\n","\n","OBjective: This lab is just to let you have a quick hands-on exposure to another example of more advanced **neural network** using the high-level library called **PyTorch**. Do not worry if you do not understand WHY it works, just play around with it to have an idea HOW it works."],"metadata":{"id":"xZ8vINH0S4j2"}},{"cell_type":"markdown","source":["## Mount Google Drive"],"metadata":{"id":"N5g_DxIWo4P0"}},{"cell_type":"code","source":["# This is needed if you need to read data from your Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_SICRjMvoB1T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import Packages"],"metadata":{"id":"EIT23eJ5pBj6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzCCniVwNTdp"},"outputs":[],"source":["# If run from the desktop, use Anaconda to install the below:\n","#    conda install pytorch torchvision\n","\n","# Setting random seeds to ensure we have the same results each time we run the code,\n","#    this is not guaranteed across PyTorch releases.\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","from torch import optim\n","\n","import numpy as np\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","from timeit import default_timer as timer\n","\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"]},{"cell_type":"markdown","metadata":{"id":"gXmCHcwKs6rd"},"source":["## Load Training Data of FMNIST Datasets from TorchVision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PCJzXv0OK1Bs"},"outputs":[],"source":["from torchvision import datasets, transforms\n","\n","mean, std = (0.5,), (0.5,)\n","\n","# Create a transform and normalise data\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize(mean, std)\n","                              ])\n","\n","# Download FMNIST training data and load training data\n","training_data = datasets.FashionMNIST('~/.pytorch/FMNIST/',\n","                                      download = True, train = True,\n","                                      transform = transform\n","                                     )"]},{"cell_type":"markdown","source":["## Understanding and Explore the Training Data\n"],"metadata":{"id":"ijzFIo3-N3Qh"}},{"cell_type":"code","source":["print(\"Size of training_data =\", len(training_data))\n","print(\"type(training_data[0][0]) =\", type(training_data[0][0]))   # image\n","print(\"type(training_data[0][1]) =\", type(training_data[0][1]))   # label (integer)\n","\n","print(\"training_data[0][0].size() =\", training_data[0][0].size())"],"metadata":{"id":"JqkYHMLdgl_s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FMNIST_labels_map = ['T-shirt','Trouser','Pullover','Dress','Coat',\n","                     'Sandal','Shirt','Sneaker','Bag','Ankle Boot']\n","\n","def get_FMNIST_label_name(label):\n","  label_name = FMNIST_labels_map[label]\n","  return label_name\n","\n","def print_FMNIST_label_name(index, label):\n","  label_name = get_FMNIST_label_name(label)\n","  print(f\"Label name for sample with index {index} = {label_name}\")\n","\n","def show_FMNIST_image(image):\n","  plt.imshow(image.squeeze(), cmap = 'gray')"],"metadata":{"id":"rUTe2WsDn2Xz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: try different number,\n","#       must be from 0 to 59999 for traing_data and from 0 to 9999 for testing_data\n","sample_image_index = 17\n","\n","sample_image, sample_image_label = training_data[sample_image_index]\n","\n","print_FMNIST_label_name(sample_image_index, sample_image_label)\n","print()\n","show_FMNIST_image(sample_image)"],"metadata":{"id":"BdDCSiq-rceX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["figure = plt.figure(figsize=(8, 8))\n","cols, rows = 3, 3\n","\n","for i in range(1, cols * rows + 1):\n","    sample_idx = torch.randint(len(training_data), size = (1,)).item()\n","    img, label = training_data[sample_idx]\n","\n","    figure.add_subplot(rows, cols, i)\n","\n","    label_name = get_FMNIST_label_name(label)\n","    plt.title(label_name)\n","\n","    plt.axis(\"off\")\n","    plt.imshow(img.squeeze(), cmap=\"gray\")\n","plt.show()"],"metadata":{"id":"OH0iJMsXrxIY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Understanding and Explore the DataLoader"],"metadata":{"id":"HZfbcqzYvYdt"}},{"cell_type":"code","source":["# Prepare Training Data with DataLoaders\n","train_dataloader = torch.utils.data.DataLoader(training_data,\n","                                               batch_size = 64,\n","                                               shuffle = False\n","                                              )\n","\n","# Load one batch of training data\n","training_batch_images, training_batch_labels = next(iter(train_dataloader))\n","\n","print(\"len(training_batch_images) =\", len(training_batch_images))\n","print(\"len(training_batch_labels) =\", len(training_batch_labels))\n","\n","print(\"type(training_batch_images) =\", type(training_batch_images))\n","print(\"type(training_batch_labels) =\", type(training_batch_labels))\n","\n","print(\"training_batch_images.shape =\", training_batch_images.shape)\n","print(\"training_batch_labels.shape =\", training_batch_labels.shape)"],"metadata":{"id":"s3nv8s7vyJY_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: try different number,\n","#       must be from 0 to 59999 for traing_data and from 0 to 9999 for testing_data\n","sample_image_index = 17\n","\n","sample_image       = training_batch_images[sample_image_index]\n","sample_image_label = training_batch_labels[sample_image_index]\n","\n","print_FMNIST_label_name(sample_image_index, sample_image_label)\n","print()\n","show_FMNIST_image(sample_image)"],"metadata":{"id":"-nR_gCwqweKf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Discussion\n","\n","Q: What is the size of an image?  \n","A:\n"],"metadata":{"id":"w9WTiPMGPsrP"}},{"cell_type":"markdown","source":["## Build a PyTorch `Sequential` Model\n","\n","> Indented block\n","\n"],"metadata":{"id":"0nnc5nUbm6d8"}},{"cell_type":"code","source":["def create_sequential_model():\n","  return nn.Sequential(nn.Flatten(start_dim = 1, end_dim = -1),\n","                       nn.Linear(784, 128),\n","                       nn.ReLU(),\n","                       nn.Linear(128,  64),\n","                       nn.ReLU(),\n","                       nn.Linear( 64,  10),\n","                       nn.LogSoftmax(dim = 1))"],"metadata":{"id":"ZkTKElYa8ijZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Build a PyTorch `nn.Module` Model\n","\n","- This is alternative to the PyTorch `Sequential` model\n","- Sequential is a subclass of `nn.Module`"],"metadata":{"id":"7o-1NtxABcip"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqMqFbIVrbFH"},"outputs":[],"source":["class FMNIST(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.fc1 = nn.Linear(784, 128)\n","    self.fc2 = nn.Linear(128,  64)\n","    self.fc3 = nn.Linear( 64,  10)\n","\n","  def forward(self, x):\n","    x = x.view(x.shape[0], -1)\n","\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    x = self.fc3(x)\n","    x = F.log_softmax(x, dim = 1)\n","\n","    return x"]},{"cell_type":"markdown","source":["## Define the Training Function"],"metadata":{"id":"NnCBO5t46Sgh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oNNyI5YRZ7H1"},"outputs":[],"source":["def train(model, batch_size):\n","\n","  print(\"Training: \", end = \"\")\n","\n","  criterion = nn.NLLLoss()\n","  optimizer = optim.SGD(model.parameters(), lr = 0.01)\n","\n","  train_dataloader = torch.utils.data.DataLoader(training_data,\n","                                                 batch_size = batch_size,\n","                                                 shuffle = False\n","                                                )\n","\n","  total_batches = int(len(training_data) / batch_size + 0.5)  # round up to integer\n","\n","  start = timer()\n","\n","  cum_loss = 0\n","\n","  for batch_num, (images, labels) in enumerate(train_dataloader, 1):\n","      optimizer.zero_grad()\n","      output = model(images)\n","      loss = criterion(output, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","      print(f\"{batch_num}/{total_batches}:{loss.item()} \", end = \"\")\n","\n","      cum_loss += loss.item()\n","\n","  training_loss = cum_loss/len(train_dataloader)\n","\n","  time_taken = timer() - start\n","  print()\n","  print(f\"Training loss = {training_loss} \", end = \"\")\n","  print(\"Time taken =\", time_taken)\n","\n","  return training_loss"]},{"cell_type":"markdown","source":["## Train a PyTorch `Sequential` Model"],"metadata":{"id":"VoW12Bx_B1ay"}},{"cell_type":"code","source":["torch.manual_seed(0)\n","np.random.seed(0)\n","\n","sequential_model = create_sequential_model()\n","\n","train(model = sequential_model, batch_size = 64)\n","train(model = sequential_model, batch_size = 64)"],"metadata":{"id":"0txEzRNc8tmR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train a PyTorch `nn.Module` Model"],"metadata":{"id":"AC-cINAICmhm"}},{"cell_type":"code","source":["torch.manual_seed(0)\n","np.random.seed(0)\n","\n","nn_module_model = FMNIST()\n","\n","train(model = nn_module_model, batch_size = 64)\n","train(model = nn_module_model, batch_size = 64)"],"metadata":{"id":"Nc-wBpa18_UW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load, Understand and Understand the Test Data of FMNIST Datasets from TorchVision"],"metadata":{"id":"W4pbGayriWnF"}},{"cell_type":"code","source":["mean, std = (0.5,), (0.5,)\n","\n","# Create a transform and normalise data\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize(mean, std)\n","                              ])\n","\n","# Download FMNIST test dataset and load test data\n","testing_data = datasets.FashionMNIST('~/.pytorch/FMNIST/',\n","                                     download = True, train = False,\n","                                     transform = transform\n","                                    )"],"metadata":{"id":"kPANiwDi-VjE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Size of testing_data =\", len(testing_data))\n","print(\"type(testing_data[0][0]) =\", type(testing_data[0][0]))   # image\n","print(\"type(testing_data[0][1]) =\", type(testing_data[0][1]))   # label (integer)\n","\n","print(\"testing_data[0][0].size() =\", testing_data[0][0].size())"],"metadata":{"id":"EZbi0rfYIeJ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test the Model with a Sample Test Data"],"metadata":{"id":"qy3Dn3hmbBff"}},{"cell_type":"code","source":["# TODO: try different number,\n","#       must be from 0 to 59999 for traing_data and from 0 to 9999 for testing_data\n","sample_image_index = 127\n","\n","sample_image, sample_image_label = testing_data[sample_image_index]\n","\n","print_FMNIST_label_name(sample_image_index, sample_image_label)\n","print()\n","show_FMNIST_image(sample_image)"],"metadata":{"id":"QfpIVzU87B4H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad(): # perform inference/evaluation without computing gradients or storing intermediate values\n","  logps = sequential_model(sample_image) # log probabilities\n","\n","ps = torch.exp(logps) # probabilities\n","\n","print(\"logps =\", logps)\n","print(\"ps =\", ps)\n","\n","nps = ps.numpy()[0] # in numpy list\n","print(\"nps =\", nps)"],"metadata":{"id":"3V5ziPeHjMRL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kBf23XrtqrB6"},"outputs":[],"source":["plt.xticks(np.arange(10), labels = FMNIST_labels_map, rotation = 'vertical')\n","\n","plt.bar(np.arange(10), nps)"]},{"cell_type":"markdown","source":["## Test the Model with All Test Data"],"metadata":{"id":"FkbIinEThk8A"}},{"cell_type":"code","source":["def test(model, batch_size):\n","\n","  print(\"Testing: \", end = \"\")\n","\n","  criterion = nn.NLLLoss()\n","\n","  test_dataloader = torch.utils.data.DataLoader(testing_data,\n","                                                batch_size = batch_size,\n","                                                shuffle = False\n","                                               )\n","\n","  start = timer()\n","\n","  cum_loss = 0\n","\n","  for batch_num, (images, labels) in enumerate(test_dataloader, 1):\n","    output = model(images)\n","    loss = criterion(output, labels)\n","\n","    cum_loss += loss.item()\n","\n","  testing_loss = cum_loss/len(test_dataloader)\n","\n","  time_taken = timer() - start\n","\n","  print(f\"Testing loss = {testing_loss} \", end = \"\")\n","  print(\"Time taken =\", time_taken)\n","\n","  return testing_loss"],"metadata":{"id":"NDqjOUXohw5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test(model = sequential_model, batch_size = 64)"],"metadata":{"id":"v9BaOigYhv85"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training and Testing"],"metadata":{"id":"wMJ8fD-ZpDlm"}},{"cell_type":"code","source":["torch.manual_seed(0)\n","np.random.seed(0)\n","\n","sequential_model = create_sequential_model()\n","\n","num_epochs = 20\n","batch_size = 64\n","\n","training_losses = []\n","testing_losses = []\n","\n","for i in range(1, num_epochs + 1):\n","\n","  print(f\"Epoch {i}/{num_epochs} => \")\n","\n","  training_loss = train(model = sequential_model, batch_size = batch_size)\n","  testing_loss  = test( model = sequential_model, batch_size = batch_size)\n","\n","  training_losses.append(training_loss)\n","  testing_losses.append(testing_loss)\n","\n","  print(f\"Epoch {i}/{num_epochs} training loss = {training_loss} testing loss = {testing_loss}\")\n","\n","print(training_losses)\n","print(testing_losses)"],"metadata":{"id":"tR7cGLD7pBtY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = list( range(1, num_epochs + 1) )\n","x_range = (1, num_epochs + 1)\n","\n","plt.xticks(range(1, num_epochs + 1, 1))\n","\n","plt.plot(x, training_losses, color = \"r\", label = \"Training Losses\")\n","plt.plot(x, testing_losses,  color = \"b\", label = \"Testing Losses\")\n","\n","plt.locator_params(axis='y', nbins=10)\n","\n","plt.xlim(x_range)\n","\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"LOsses\")\n","plt.title(\"Training & Testing Losses\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"ffLsmi77pBaC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Working with GPUs\n","\n","In Google Colab, go to the menu \"Runtime -> Change run time type\" to set the \"Hardware accelerator\" to \"GPU\""],"metadata":{"id":"uU-OY_EKVQ1p"}},{"cell_type":"code","source":["# NVIDIA (R) Cuda compiler driver\n","!nvcc --version"],"metadata":{"id":"9I1M4q3BXQof"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# NVIDIA System Management Interface (nvidia-smi)\n","!nvidia-smi"],"metadata":{"id":"Jv9yumDZXSDl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Refer to https://pytorch.org/get-started/locally/\n","# conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"id":"1q3Ifpj5ybH6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, batch_size, use_gpu = False):\n","\n","  print(\"use_gpu =\", use_gpu)\n","  print(\"Training: \", end = \"\")\n","\n","  criterion = nn.NLLLoss()\n","  optimizer = optim.SGD(model.parameters(), lr = 0.01)\n","\n","  train_dataloader = torch.utils.data.DataLoader(training_data,\n","                                                 batch_size = batch_size,\n","                                                 shuffle = False\n","                                                )\n","\n","  total_batches = int(len(training_data) / batch_size + 0.5)  # round up to integer\n","\n","  start = timer()\n","\n","  cum_loss = 0\n","\n","  for batch_num, (images, labels) in enumerate(train_dataloader, 1):\n","      optimizer.zero_grad()\n","\n","      if (use_gpu == True):\n","        # Move data to GPU\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","      output = model(images)\n","      loss = criterion(output, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","      print(f\"{batch_num}/{total_batches}:{loss.item()} \", end = \"\")\n","\n","      cum_loss += loss.item()\n","\n","  training_loss = cum_loss/len(train_dataloader)\n","\n","  time_taken = timer() - start\n","  print()\n","  print(f\"Training loss = {training_loss} \", end = \"\")\n","  print(\"Time taken =\", time_taken)\n","\n","  return training_loss"],"metadata":{"id":"sfCmusoyKPGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(0)\n","np.random.seed(0)\n","\n","sequential_model_gpu = create_sequential_model()\n","\n","# Move model to GPU\n","sequential_model_gpu.to(device)\n","\n","# Train model in GPU\n","train(model = sequential_model_gpu, batch_size = 64, use_gpu = True)\n","train(model = sequential_model_gpu, batch_size = 64, use_gpu = True)"],"metadata":{"id":"KLv13xlIzzvt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: try different number,\n","#       must be from 0 to 59999 for traing_data and from 0 to 9999 for testing_data\n","sample_image_index = 127\n","\n","sample_image, sample_image_label = testing_data[sample_image_index]\n","\n","print_FMNIST_label_name(sample_image_index, sample_image_label)\n","print()\n","show_FMNIST_image(sample_image)\n","\n","# Move model back to CPU\n","sequential_model_gpu.to(\"cpu\")\n","\n","with torch.no_grad(): # perform inference/evaluation without computing gradients or storing intermediate values\n","  logps = sequential_model_gpu(sample_image) # log probabilities\n","\n","ps = torch.exp(logps) # probabilities\n","\n","print(\"logps =\", logps)\n","print(\"ps =\", ps)\n","\n","nps = ps.numpy()[0] # in numpy list\n","print(\"nps =\", nps)\n","\n","plt.xticks(np.arange(10), labels = FMNIST_labels_map, rotation = 'vertical')\n","\n","plt.bar(np.arange(10), nps)\n"],"metadata":{"id":"2ZPrrJfVV8wi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step Through Sequential Model"],"metadata":{"id":"Hjmf2O1Z8vph"}},{"cell_type":"code","source":["def print_weights(model, index):\n","  print(f\"model[{index}].weight =>\", model[index].weight)\n","  print(f\"model[{index}].weight.grad =>\", model[index].weight.grad)"],"metadata":{"id":"lLbIiqfzw3Xf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(0)\n","np.random.seed(0)\n","\n","train_dataloader = torch.utils.data.DataLoader(training_data,\n","                                               batch_size = 64,\n","                                               shuffle = False\n","                                              )\n","\n","# Load one batch of training data\n","images, labels = next(iter(train_dataloader))\n","\n","print(\"images.shape =\", images.shape)"],"metadata":{"id":"3ukIRlws9Wap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mini_sequential_model = create_sequential_model()\n","\n","criterion = nn.NLLLoss()\n","optimizer = optim.SGD(mini_sequential_model.parameters(), lr = 0.01)\n","\n","print_weights(mini_sequential_model, 1)"],"metadata":{"id":"JHkLZ5b5SUek"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = mini_sequential_model(images)\n","loss = criterion(output, labels)\n","loss.backward()\n","\n","print(f\"loss.item() = {loss.item()} \")\n","\n","print_weights(mini_sequential_model, 1)"],"metadata":{"id":"M1kM88936KfI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer.step()\n","\n","print_weights(mini_sequential_model, 1)"],"metadata":{"id":"PI8De39E6KJQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer.zero_grad()\n","\n","print_weights(mini_sequential_model, 1)"],"metadata":{"id":"L6rOXkvpY4_1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = mini_sequential_model(images)\n","loss = criterion(output, labels)\n","loss.backward()\n","optimizer.step()\n","\n","print(f\"loss.item() = {loss.item()} \")\n","print_weights(mini_sequential_model, 1)"],"metadata":{"id":"4LCDbzwIY8fl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer.zero_grad()\n","output = mini_sequential_model(images)\n","loss = criterion(output, labels)\n","loss.backward()\n","optimizer.step()\n","\n","print(f\"loss.item() = {loss.item()} \")\n","print_weights(mini_sequential_model, 1)"],"metadata":{"id":"2xBzpk6-ZHEt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(mini_sequential_model[0])\n","print(mini_sequential_model[1])\n","print(mini_sequential_model[2])\n","print(mini_sequential_model[3])\n","print(mini_sequential_model[4])\n","print(mini_sequential_model[5])\n","print(mini_sequential_model[6])\n"],"metadata":{"id":"w4IwZJOKg_en"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dpF-wPx_xmmo"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}